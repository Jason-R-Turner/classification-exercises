{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54fa5d99-f1cb-4db7-81a8-6485e38e0229",
   "metadata": {},
   "source": [
    "# Acquire Data for Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7dae69-547f-4d40-bda3-f3bd1f2e99b7",
   "metadata": {},
   "source": [
    "# Big Ideas\n",
    "- Cache your data to speed up your data acquisition.\n",
    "- Helper functions are your friends.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b3ee4d-9698-45bd-a06f-42e0040450b4",
   "metadata": {},
   "source": [
    "# Objectives\n",
    "By the end of the acquire lesson and exercises, you will be able to...\n",
    "- read data into a pandas DataFrame using the following modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf412f6-03f7-4651-8cdd-f4873bc8a7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pydataset\n",
    "\n",
    "# from pydataset import data\n",
    "# df = data('dataset_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2f0595-7286-49d5-9010-1c6032429c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # seaborn datasets\n",
    "\n",
    "# import seaborn as sns\n",
    "# df = sns.load_dataset('dataset_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439eaa9b-2f72-4f02-845c-05a0c96e91a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# visualize\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.rc('figure', figsize=(8, 6))\n",
    "plt.rc('font', size=13)\n",
    "\n",
    "# turn off pink warning boxes\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# acquire\n",
    "from env import host, user, password\n",
    "\n",
    "# To access pydataset data table use:\n",
    "from pydataset import data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f0f1ac-d96a-415a-84ee-d07d3288c9e3",
   "metadata": {},
   "source": [
    "4. In a jupyter notebook, `classification_exercises.ipynb`, use a python module (pydata or seaborn datasets) containing datasets as a source from the iris data. Create a pandas dataframe, `df_iris`, from this data.\n",
    "- print the first 3 rows\n",
    "- print the number of rows and columns (shape)\n",
    "- print the column names\n",
    "- print the data type of each column\n",
    "- print the summary statistics for each of the numeric variables\n",
    "\n",
    "## 4. Create `df_iris`\n",
    "\n",
    "- Use a python module (pydata or seaborn datasets) containing datasets as a source for the iris data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2127b452-5d71-4261-badf-6d39383dfa16",
   "metadata": {},
   "outputs": [],
   "source": [
    "data('iris', show_doc=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b0e01d-4789-4e84-b397-3486ae1b5ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using pydataset\n",
    "\n",
    "df_iris = data('iris')\n",
    "df_iris.head(1)\n",
    "\n",
    "# Does pydatataset not have the range column like seaborn does?  \n",
    "# Also does it capitaliing column names affect anything?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414d51dd-807e-4e06-9401-bece744237b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using seaborn -- love the column names.\n",
    "\n",
    "df_iris = sns.load_dataset('iris')\n",
    "df_iris.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c05a420e-a6d3-421b-8d93-0a7d89fce866",
   "metadata": {},
   "source": [
    "### Print the first 3 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74ee45d-69c8-4894-902a-5c8b195491a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_iris.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e897e08-38de-4e0f-9078-9a4bb5b83979",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_iris.iloc[0:3]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ca8e476f-c7b3-4817-97f1-80518765a544",
   "metadata": {},
   "source": [
    "### Print the number of rows and columns (shape)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3e5178-4945-4419-bbf7-bc0018c88e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_iris.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a198182-bfb1-490b-81ba-ca9c0247df6a",
   "metadata": {},
   "source": [
    "--------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136e52ff-861f-4d50-957f-83bc8d3b3c63",
   "metadata": {},
   "source": [
    "### Print the column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e2bd2c-0c83-4a8c-8e78-ff8564aab9a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_iris.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa78d2de-d772-40a8-81fd-461c0dc05418",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return a nice list of coluns if I want to grab and use them later.\n",
    "\n",
    "df_iris.columns.to_list()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d97bfa0-e0ae-48dc-a2a7-9a5eedf29d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in df_iris.columns:\n",
    "    print(column)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9fff335-43e9-430f-bce3-b0fc3cef0e15",
   "metadata": {},
   "source": [
    "## Print the data type of each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28cf5d8-3fec-434c-8bcb-45678a724943",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return just data types.\n",
    "\n",
    "df_iris.dtypes # For one data type it's just 'dtype'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777c43b6-c773-4e3d-9138-4c507a442a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_iris.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58731b5-4dde-4757-865e-05bb2cead618",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This method returns the summary statistics for numeric variable in my df.\n",
    "\n",
    "stats = df_iris.describe().T\n",
    "stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede3e371-cd1c-4466-98f1-2c4e565d75bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I can calculate a range for each numeric variable and select certain columns of interest.\n",
    "\n",
    "stats['range'] = stats['max'] - stats['min']\n",
    "stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8acbd6b0-46d0-48d9-a77c-ba5504640b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats[['mean', '50%', 'std']]\n",
    "# Use double brackets to make a list of the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224d4504-836a-44bf-8c3f-1c1fe625795b",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_of_columns = ['mean', '50%', 'std']\n",
    "stats[subset_of_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de8e4b3-8644-4f1b-b3ee-7a919fb7a1c5",
   "metadata": {},
   "source": [
    "5. Read the Table1_CustDetails table from your spreadsheet exercises google sheet into a dataframe named df_google_sheets.\n",
    "\n",
    "Make sure that the spreadsheet is publicly visible under your sharing settings.\n",
    "- assign the first 100 rows to a new dataframe, df_google_sheets_sample\n",
    "- print the number of rows of your original dataframe\n",
    "- print the first 5 column names\n",
    "- print the column names that have a data type of object\n",
    "- compute the range for each of the numeric variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9877aa07-1cfe-4355-949c-c6d6b4d116a4",
   "metadata": {},
   "source": [
    "## Create `df_google`\n",
    "- Read the data from a Google sheet into a dataframe, df_google."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff91a10-da5e-4b71-8b97-77028b206158",
   "metadata": {},
   "outputs": [],
   "source": [
    "sheet_url = 'https://docs.google.com/spreadsheets/d/1kcrY0Q2IGFaEg0OgWxJORGCC0tNjH-L42Z0Q-4ajIUY/edit#gid=1023018493'\n",
    "# Grabbed the Sheets URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a302a50-bb66-4bec-bdad-ae6b6e03a5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_export_url = sheet_url.replace('/edit#gid=', '/export?format=csv&gid=')\n",
    "# Turns the Sheets address into a CSV export URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e31223-aada-4a99-b497-a666da303d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_google = pd.read_csv(csv_export_url)\n",
    "df_google\n",
    "# Uses the pandas '`pd.read_csv()` function to read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d51d37-dac8-4720-8fe0-039842adb730",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the first 3 rows.\n",
    "\n",
    "df_google.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d86eaeb-a61e-4647-a1b1-7cf268edc3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the number of rows and columns.\n",
    "df_google.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0d48af-d489-49ca-9342-8b8e4d245a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the column names.\n",
    "df_google.columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895edf94-379f-4c55-91dd-eff8875347e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the data type of each column.\n",
    "df_google.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2e9df9-594c-4124-b3e3-66847732e51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the sumary statistics for each of the numeric variables.\n",
    "df_google.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54121e74-c0a7-48bc-8720-9bf0585f6180",
   "metadata": {},
   "source": [
    "## Print the unique values for each of your categorical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14151a0-d6dc-455e-9ad1-b88c91c94f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df_google.columns:\n",
    "    \n",
    "        if df_google[col].dtypes == 'object':\n",
    "            print(f'{col} has {df_google[col].nunique()} unique vlaues.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9104d516-4fa2-49c2-a4de-e578b4866dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df_google.columns:\n",
    "    if df_google[col].dtypes == 'object':\n",
    "        print(df_google[col].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8cb361-4943-455e-98bc-2a9852013d5e",
   "metadata": {},
   "source": [
    "6. Download your spreadsheet exercises google sheet as an excel file (File → Download → Microsoft Excel). Read the Table1_CustDetails worksheet into a dataframe named df_excel.\n",
    "- assign the first 100 rows to a new dataframe, df_excel_sample\n",
    "- print the number of rows of your original dataframe\n",
    "- print the first 5 column names\n",
    "- print the column names that have a data type of object\n",
    "- compute the range for each of the numeric variables.\n",
    "\n",
    "## 6. Create `df_excel`\n",
    "- Read the `Table1_CustDetails` table from the `Excel_Exercises.xlsx`, sheet_name='Table1_CustDetails')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca977a7-e147-44a3-90d2-ad1bb80299fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "help(pd.read_excel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08af7a21-1d29-4d2d-a61e-cde76df693dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_excel = pd.read_excel('Jason Turner - jemison_spreadsheet_exercises.xlsx', sheet_name='Table1_CustDetails')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb05d7b6-519c-413d-bb05-a8cea8ed2d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign the first 100 rows to a new dataframe, `df_excel_sample`.\n",
    "df_excel.iloc[0:100]\n",
    "df_excel_sample = df_excel.head(100)\n",
    "df_excel_sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a072146a-f716-4b91-9f04-8d9f2333123f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the number of rows of your original dataframe.\n",
    "df_excel.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e868064b-2da3-448c-89f5-15ac724ada59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the first 5 column names.\n",
    "df_excel.columns[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53001c4c-acb2-45d8-acf8-4d17f48ebc97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the column names that have a data type of object.\n",
    "df_excel.select_dtypes(include='object').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf1c8ec-3ad0-4b7e-940a-9e4f1d96e242",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_excel.select_dtypes(include='object').columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3efed30f-b17d-4784-a49e-ffe882507850",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_excel.select_dtypes(include=['object', 'int64']).head()\n",
    "# You can pass of list of the data types that you want to include or exclude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1c2cbb-d981-4548-a452-bc77eabb91ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What if we want to exclude floats\n",
    "\n",
    "df_excel.select_dtypes(exclude=['float64']).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2dd6b86-8b51-4f66-88d9-1e5ec1e2f6ea",
   "metadata": {},
   "source": [
    "### Compute the range for each of the numeric variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9ff5fc-109a-430b-aad6-2f1a6a907ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some of these numeric columns are more like encoded categorical variables.\n",
    "\n",
    "df_excel.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e68c98-fcae-4851-bccf-6a4bd99a5fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I can select just the true numeric variables to declutter my results.\n",
    "\n",
    "telco_stats = df_excel[['monthly_charges', 'total_charges']].describe().T\n",
    "telco_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23857c09-f8d0-4996-af29-7495d4f01b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "telco_stats['range'] = telco_stats['max'] - telco_stats['min']\n",
    "telco_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a52ea2-3ece-4e6f-9970-5ebba3595320",
   "metadata": {},
   "source": [
    "7. Read the data from this google sheet into a dataframe, df_google.\n",
    "- print the first 3 rows\n",
    "- print the number of rows and columns\n",
    "- print the column names\n",
    "- print the data type of each column\n",
    "- print the summary statistics for each of the numeric variables\n",
    "- print the unique values for each of your categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d263d809-a38b-4e75-bede-94d3d661f54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sheet_url = 'https://docs.google.com/spreadsheets/d/1Uhtml8KY19LILuZsrDtlsHHDC9wuDGUSe8LTEwvdI5g/edit#gid=341089357'\n",
    "csv_export_url = sheet_url.replace('/edit#gid=', '/export?format=csv&gid=')\n",
    "df_google = pd.read_csv(csv_export_url)\n",
    "print(df_google.head(3))\n",
    "print(df_google.shape)\n",
    "print(df_google.columns.to_list())\n",
    "print(df_google.info)\n",
    "print(df_google.describe())\n",
    "for col in df_google.columns:\n",
    "    if df_google[col].dtypes == 'object':\n",
    "        print(f'{col} has {df_google[col].nunique()} unique values.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb2aa5d-2651-4436-8c10-31b08fc5082a",
   "metadata": {},
   "source": [
    "Make a new python module, acquire.py\n",
    "\n",
    "**Make sure your `env.py` and csv files are *not* being pushed to GitHub!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47fcff1a-0462-4365-a418-ec957d543209",
   "metadata": {},
   "source": [
    "# Exercise 1 for `acquire.py`\n",
    "Make a function named `get_titanic_data` that returns the titanic data from the codeup data science database as a pandas data frame. Obtain your data from the Codeup Data Science Database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5057f236-fd0c-4169-a3d0-435c45be5136",
   "metadata": {},
   "outputs": [],
   "source": [
    "import acquire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8da3f1f-7064-46b0-b88d-ad6468ffe6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_df = acquire.get_titanic_data()\n",
    "titanic_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b1fd9d-ef17-4f6c-b489-9ab2bb0ccb39",
   "metadata": {},
   "source": [
    "## Exercise 2 for `acquire.py`\n",
    "\n",
    "Make a function named get_iris_data that returns the data from the iris_db on the codeup data science database as a pandas data frame. The returned data frame should include the actual name of the species in addition to the species_ids. Obtain your data from the Codeup Data Science Database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7462a035-db68-49ab-8010-1d5f24203595",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_df = acquire.get_iris_data()\n",
    "iris_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a1b3d9-7dab-4ee5-83d3-b6a7b303dbe1",
   "metadata": {},
   "source": [
    "### Exercise 3 for `acquire.py`\n",
    "\n",
    "Make a function named get_telco_data that returns the data from the telco_churn database in SQL. In your SQL, be sure to join all 4 tables together, so that the resulting dataframe contains all the contract, payment, and internet service options. Obtain your data from the Codeup Data Science Database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d3cd35-a5a6-4b42-8a35-6d566b18f15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "telco_df = acquire.get_telco_data()\n",
    "telco_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9c1799-a7da-4db8-b716-6269c5baf613",
   "metadata": {},
   "source": [
    "### Add Caching to the `acquire.py` functions\n",
    "\n",
    "Once you've got your get_titanic_data, get_iris_data, and get_telco_data functions written, now it's time to add caching to them. To do this, edit the beginning of the function to check for the local filename of telco.csv, titanic.csv, or iris.csv. If they exist, use the .csv file. If the file doesn't exist, then produce the SQL and pandas necessary to create a dataframe, then write the dataframe to a .csv file with the appropriate name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030c7768-caa5-409e-8592-737a6b151347",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if os.path.isfile('titanic_df.csv'):\n",
    "        \n",
    "#         # If csv file exists, read in data from csv file.\n",
    "#         df = pd.read_csv('titanic_df.csv', index_col=0)\n",
    "        \n",
    "#     else:\n",
    "        \n",
    "#         # Read fresh data from db into a DataFrame.\n",
    "#         df = new_titanic_data()\n",
    "        \n",
    "#         # Write DataFrame to a csv file.\n",
    "#         df.to_csv('titanic_df.csv')\n",
    "        \n",
    "# if os.path.isfile('iris_df.csv'):\n",
    "        \n",
    "#         # If csv file exists read in data from csv file.\n",
    "#         df = pd.read_csv('iris_df.csv', index_col=0)\n",
    "        \n",
    "#     else:\n",
    "        \n",
    "#         # Read fresh data from db into a DataFrame\n",
    "#         df = new_iris_data()\n",
    "        \n",
    "#         # Cache data\n",
    "#         df.to_csv('iris_df.csv')\n",
    "        \n",
    "# if os.path.isfile('telco.csv'):\n",
    "        \n",
    "#         # If csv file exists read in data from csv file.\n",
    "#         df = pd.read_csv('telco.csv', index_col=0)\n",
    "        \n",
    "#     else:\n",
    "        \n",
    "#         # Read fresh data from db into a DataFrame\n",
    "#         df = new_telco_data()\n",
    "        \n",
    "#         # Cache data\n",
    "#         df.to_csv('telco.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04566791-9c89-413f-8dd3-a1faaedf5574",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0253069e-6540-4d9b-b309-70198ba574e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# visualize\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.rc('figure', figsize=(8, 6))\n",
    "plt.rc('font', size=13)\n",
    "\n",
    "# turn off pink warning boxes\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# acquire\n",
    "from env import host, user, password\n",
    "\n",
    "# To access pydataset data table use:\n",
    "from pydataset import data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c9dd25-b420-4136-a569-5a5870beb7f7",
   "metadata": {},
   "source": [
    "##### Use the Iris Data to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9bde074b-589e-4346-a5bb-91b921ab0bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df9f40d-7b11-4f75-996a-98d5174b22dd",
   "metadata": {},
   "source": [
    "1. Use the function defined in acquire.py to load the iris data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18558cce-dbc1-403c-84eb-f857766b6f4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>species_id</th>\n",
       "      <th>species_name</th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>setosa</td>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>setosa</td>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>setosa</td>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>setosa</td>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>setosa</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   species_id species_name  sepal_length  sepal_width  petal_length  \\\n",
       "0           1       setosa           5.1          3.5           1.4   \n",
       "1           1       setosa           4.9          3.0           1.4   \n",
       "2           1       setosa           4.7          3.2           1.3   \n",
       "3           1       setosa           4.6          3.1           1.5   \n",
       "4           1       setosa           5.0          3.6           1.4   \n",
       "\n",
       "   petal_width  \n",
       "0          0.2  \n",
       "1          0.2  \n",
       "2          0.2  \n",
       "3          0.2  \n",
       "4          0.2  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import acquire\n",
    "iris_df = acquire.get_iris_data()\n",
    "iris_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409cbb67-9895-40bf-b392-b7c588081989",
   "metadata": {},
   "source": [
    "2. Drop the `species_id` and `measurement_id` columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16740017-e6d8-4144-b007-61d4067f1813",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def clean_iris(df):\n",
    "#     '''\n",
    "    \n",
    "#     '''\n",
    "#     df = df.drop_duplicates()\n",
    "#     df = df.drop(columns=['species_id', 'measurement_id'])\n",
    "    \n",
    "#     dummy_df = pd.get_dummies(df['species'], drop_first=False)\n",
    "\n",
    "#     df = pd.concat([df, dummy_df], axis=1)\n",
    "\n",
    "#     return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90c73ec-a136-4e12-85b8-798ec9838ea7",
   "metadata": {},
   "source": [
    "3. Rename the `species_name` column to just `species`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2caacc7-aad3-43e7-94e4-9dfcc6e29dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_iris(df):\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    df = df.drop_duplicates()\n",
    "    df = df.drop(columns=['species_id', 'measurement_id'])\n",
    "    df = df.rename(columns= {'species_name':'species'})\n",
    "    dummy_df = pd.get_dummies(df['species'], drop_first=False)\n",
    "\n",
    "    df = pd.concat([df, dummy_df], axis=1)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "816e6eeb-0fb7-4e6d-a298-2bef04643cf6",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['measurement_id'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/tg/c6tcjwk13mj583lxrf25k3jm0000gn/T/ipykernel_36381/3123749242.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mclean_iris\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miris_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/tg/c6tcjwk13mj583lxrf25k3jm0000gn/T/ipykernel_36381/742520256.py\u001b[0m in \u001b[0;36mclean_iris\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m      4\u001b[0m     '''\n\u001b[1;32m      5\u001b[0m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop_duplicates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'species_id'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'measurement_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'species_name'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'species'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mdummy_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_dummies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'species'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop_first\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/homebrew/anaconda3/lib/python3.9/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/homebrew/anaconda3/lib/python3.9/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   4904\u001b[0m                 \u001b[0mweight\u001b[0m  \u001b[0;36m1.0\u001b[0m     \u001b[0;36m0.8\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4905\u001b[0m         \"\"\"\n\u001b[0;32m-> 4906\u001b[0;31m         return super().drop(\n\u001b[0m\u001b[1;32m   4907\u001b[0m             \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4908\u001b[0m             \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/homebrew/anaconda3/lib/python3.9/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   4148\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4149\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4150\u001b[0;31m                 \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_drop_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4152\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/homebrew/anaconda3/lib/python3.9/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_drop_axis\u001b[0;34m(self, labels, axis, level, errors)\u001b[0m\n\u001b[1;32m   4183\u001b[0m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4184\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4185\u001b[0;31m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4186\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0maxis_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnew_axis\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/homebrew/anaconda3/lib/python3.9/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, errors)\u001b[0m\n\u001b[1;32m   6015\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6016\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0merrors\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6017\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{labels[mask]} not found in axis\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6018\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6019\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['measurement_id'] not found in axis\""
     ]
    }
   ],
   "source": [
    "clean_iris(iris_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e37e692-acef-4a8a-a645-dc9e1bd997ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_iris(df):\n",
    "    '''\n",
    "    take in a Data Frame and return train, validate, and test DataFrames; stratify on species.\n",
    "    return train, validate, test DataFrames.\n",
    "    '''\n",
    "    \n",
    "    #\n",
    "    train, validate, test = train_test_split(df, test_size=.2, random_state=123, stratify=df.species)\n",
    "    \n",
    "    #\n",
    "    train, validate = train_test_split(train_validate,\n",
    "                                       \n",
    "                                       stratify=train_valdiate.species)\n",
    "    train, validate, test = split_iris_data(df)\n",
    "    \n",
    "    return train, validate, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1fb9a346-09d5-4111-a653-39a5b670687e",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'species'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/tg/c6tcjwk13mj583lxrf25k3jm0000gn/T/ipykernel_36381/3199758568.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msplit_iris\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miris_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/tg/c6tcjwk13mj583lxrf25k3jm0000gn/T/ipykernel_36381/2642937816.py\u001b[0m in \u001b[0;36msplit_iris\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m123\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstratify\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspecies\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/homebrew/anaconda3/lib/python3.9/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5485\u001b[0m         ):\n\u001b[1;32m   5486\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5487\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5488\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5489\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'species'"
     ]
    }
   ],
   "source": [
    "split_iris(iris_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a59a73-a5e7-4550-97b2-d00bbe7a0ddd",
   "metadata": {},
   "source": [
    "4. Create dummy variables of the species name and concatenate onto the iris dataframe. (This is for practice, we don't always have to encode the target, but if we used species as a feature, we would need to encode it)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9462c5f-958d-47cb-ad18-3c6669b538f0",
   "metadata": {},
   "source": [
    "5. Create a function named `prep_iris` that accepts the untransformed iris data, and returns the data with the transformations above applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a489f3d3-c4bb-4f16-bd21-d2769ff1aec0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prep_iris_data(df):\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    df = clean_iris(df)\n",
    "    train, validate, test = split_iris(df)\n",
    "    returner train, validate, test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907b4f90-2df9-4600-9689-2ffd342532ad",
   "metadata": {},
   "source": [
    "##### Use the Titanic dataset to:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57023c83-3290-46b2-9231-558f0a9d7c3b",
   "metadata": {},
   "source": [
    "1. Use the function defined in `acquire.py` to load the Titanic data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd39303d-c9fc-4789-a6e7-cda80dbb51d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import acquire\n",
    "titanic_df = acquire.get_titanic_data()\n",
    "titanic_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c00c4c4-be1c-4e30-919d-5d0ed2fbaeab",
   "metadata": {},
   "source": [
    "2. Drop any unnecessary, unhelpful, or duplicated columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72aae081-61a9-4653-9d7c-09aebe10d29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_titanic(df):\n",
    "    '''\n",
    "    This function will clean the data prior to splitting.\n",
    "    '''\n",
    "    # Drops any duplicate values\n",
    "    df = df.drop_duplicates()\n",
    "    \n",
    "    # Drops columns that are already represented by other columns\n",
    "    cols_to_drop = ['deck', 'embarked', 'class']\n",
    "    dr = df.drop(columns = cols_to_drop)\n",
    "    \n",
    "    # Fills the small number of null values for embark_town with the mode\n",
    "    df['embark_town'] = df.embark_town.fillna(value='Southampton')\n",
    "    \n",
    "    # Uses one-hot encoding to create dummies of string columns for future modeling\n",
    "    dummy_df = pd.get_dummies(df[['sex', 'embark_town']], dummy_na=False, drop_first=[True, True])\n",
    "    df = pd.concat([df, dummy_df], axis=1)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8c68d6-5b65-4a24-b771-341cca53ac3b",
   "metadata": {},
   "source": [
    "3. Encode the categorical columns. Create dummy variables of the categorical columns and concatenate them onto the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c08f32-4fb7-4b06-b6eb-0a42ab800a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_titanic(df):\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    #\n",
    "    train, test = train_test_split(df, test_size = .2, random_state=123, stratify=df.survived)\n",
    "    \n",
    "    #\n",
    "    train, validate = train_test_split(train, test_size=.3, random_state=123, stratify=train.survived)\n",
    "    \n",
    "    return train, validate, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49e5a13-394a-4947-86d1-64cdb62aec7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_titanic(titanic_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f586878-ffb7-45a5-a344-ffcc4b32f252",
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_titanic_mode(train, validate, test):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34dbfbf7-98c2-4260-919c-f27bfcf3c849",
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_mean_age(train, validate, test):\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32317370-be54-48f6-935f-ebf2843ec9bc",
   "metadata": {},
   "source": [
    "4. Create a function named `prep_titanic` that accepts the raw titanic data, and returns the data with the transformations above applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0fd7a23-8fae-450c-9ddf-172fa9491211",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_titanic(df):\n",
    "    '''\n",
    "    This function takes in a df and will drop any duplicate observations, \n",
    "    drop ['deck', 'embarked', 'class', 'age'], fill missing embark_town with 'Southampton'\n",
    "    create dummy vars from sex and embark_town, and perform a train, validate, test split. \n",
    "    Returns train, validate, and test DataFrames\n",
    "    '''\n",
    "    df = clean_titanic(df)\n",
    "    train, validate, test = split_titanic(df)\n",
    "    return train, validate, test\n",
    "\n",
    "prep_titanic(titanic_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf60f76-46c2-45cb-8d19-f2d00e06760b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db987555-892b-4efd-9a88-7358d21d6555",
   "metadata": {},
   "source": [
    "##### Use the Telco dataset to:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e043c96-52f2-4625-90f1-b66ae814a98a",
   "metadata": {},
   "source": [
    "1. Use the function defined in `acquire.py` to load the Telco data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8396e947-1e2e-4f63-987c-ddf86fd6a4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import acquire\n",
    "telco_df = acquire.get_telco_data()\n",
    "telco_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6340a13f-6d5b-46ee-8363-bc84ad3997b1",
   "metadata": {},
   "source": [
    "2. Drop any unnecessary, unhelpful, or duplicated columns. This could mean dropping foreign key columns but keeping the corresponding string values, for example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c668ef-a42b-4ef0-945f-cabdfe3af523",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def clean_telco(df):\n",
    "    '''\n",
    "    This function will clean the data prior to splitting.\n",
    "    '''\n",
    "    # Drops any duplicate values\n",
    "    df = df.drop_duplicates()\n",
    "    \n",
    "    # Drop duplicate columns\n",
    "    df.drop(columns=['payment_type_id', 'internet_service_type_id', 'contract_type_id', 'customer_id'], inplace=True)\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72351b03-91ac-4775-8cce-ca18164f58e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_telco(telco_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55dacd5f-406c-4339-9911-a06238448f8f",
   "metadata": {},
   "source": [
    "3. Encode the categorical columns. Create dummy variables of the categorical columns and concatenate them onto the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2fcf50c-6251-4ef1-bfae-ee97ac40de93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_telco(df):\n",
    "    '''\n",
    "    This function will take in a DataFrame and return train, validate, and test DataFrames; stratify on total_charges \n",
    "    return train, validate, test DataFrames\n",
    "    '''\n",
    "    \n",
    "    #\n",
    "    train, test = train_test_split(df, test_size = .2, random_state=123, stratify=df.senior_citizen)\n",
    "    \n",
    "    #\n",
    "    train, validate = train_test_split(train, test_size=.3, random_state=123, stratify=train.senior_citizen)\n",
    "    \n",
    "    return train, validate, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae448b02-512a-4849-801a-3311779eb055",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_telco(telco_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18da63e-4719-46c7-9d5a-9c42a0455333",
   "metadata": {},
   "source": [
    "4. Create a function named `prep_telco` that accepts the raw telco data, and returns the data with the transformations above applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90cd512b-4c2b-484a-9503-6068d138de3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_telco(df):\n",
    "    # Drop duplicate columns\n",
    "    # df.drop(columns=['payment_type_id', 'internet_service_type_id', 'contract_type_id', 'customer_id'], inplace=True)\n",
    "    \n",
    "    # Drop null values stored as whitespace\n",
    "    df['total_charges'] = df['total_charges'].str.strip()\n",
    "    df = df[df.total_charges != '']\n",
    "    \n",
    "    # Convert to correct datatype\n",
    "    df['total_charges'] = df.total_charges.astype(float)\n",
    "    \n",
    "    # Convert binary categorical variables to numeric.  It's similar to using one-hot\n",
    "    df['gender_encoded'] = df.gender.map({'Female': 1, 'Male': 0})\n",
    "    df['partner_encoded'] = df.partner.map({'Yes': 1, 'No': 0})\n",
    "    df['dependents_encoded'] = df.dependents.map({'Yes': 1, 'No': 0})\n",
    "    df['phone_service_encoded'] = df.phone_service.map({'Yes': 1, 'No': 0})\n",
    "    df['churn_endcoded'] = df.churn.map({'Yes': 1, 'No': 0})\n",
    "    \n",
    "    # Get dummies for non-binary categorical variables\n",
    "    dummy_df = pd.get_dummies(df[['multiple_lines', \\\n",
    "                                  'online_security', \\\n",
    "                                  'online_backup', \\\n",
    "                                  'tech_support', \\\n",
    "                                  'streaming_tv', \\\n",
    "                                  'streaming_movies', \\\n",
    "                                  'contract_type', \\\n",
    "                                  'internet_service_type', \\\n",
    "                                  'payment_type']], dummy_na=False, \\\n",
    "                                  drop_first=True)\n",
    "    \n",
    "    # Concatenate dummy dataframe to original\n",
    "    df = pd.concat([df, dummy_df], axis=1)\n",
    "    \n",
    "    # split the data\n",
    "    train, validate, test = split_telco(df)\n",
    "    \n",
    "    return train, validate, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3c9230-9b38-47dc-a2f9-a8f698de3d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "prep_telco(telco_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1669cd67-b376-468c-bb96-9568609c6fac",
   "metadata": {},
   "source": [
    "# Exploratory Analysis\n",
    "### Exercises Part I"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172485b2-7d78-4487-8ec0-4b850c60f881",
   "metadata": {},
   "source": [
    "#### Section 1 - iris_db:\n",
    "1. Acquire, prepare & split your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bee5d5e-a89b-4e83-97a1-098a180789de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import acquire\n",
    "import prepare\n",
    "iris_df = acquire.get_iris_data()\n",
    "iris_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ffe0fc0-6e63-4e8e-ba81-8d156d06c60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prep_iris(iris_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c60714-0ec0-47fe-bb30-c04fda174b67",
   "metadata": {},
   "source": [
    "2. Univariate Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c255275f-3017-4bba-af4a-6ce962fe9894",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4b94c64c-50ab-41d4-b1c7-e823f6ade316",
   "metadata": {},
   "source": [
    "- For each measurement type (quantitative variable): create a histogram, boxplot, & compute descriptive statistics (using .describe())."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8b69c2-c11c-4a23-9c17-3e815e649f72",
   "metadata": {},
   "source": [
    "- For each species (categorical variable): create a frequency table and a bar plot of those frequencies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8566895-6ad5-4ea6-b237-51aada20e836",
   "metadata": {},
   "source": [
    "- Document takeaways & any actions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9777b937-e0aa-4eae-b340-f2305f4ca4e8",
   "metadata": {},
   "source": [
    "3. Bivariate Stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89232e1f-56da-40e2-b782-10e092e7a804",
   "metadata": {},
   "source": [
    "- Visualize each measurement type (y-axis) with the species variable (x-axis) using barplots, adding a horizontal line showing the overall mean of the metric (y-axis)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2193c6e-f336-441b-bf38-dc5f9204ae0f",
   "metadata": {},
   "source": [
    "- For each measurement type, compute the descriptive statistics for each species."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ccb2f9f-a413-4d97-8778-77cb7a750b47",
   "metadata": {},
   "source": [
    "- For virginica & versicolor: Compare the mean petal_width using the Mann-Whitney test (scipy.stats.mannwhitneyu) to see if there is a significant difference between the two groups. Do the same for the other measurement types."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c8abf7-9ea1-4911-8600-814831078fe4",
   "metadata": {},
   "source": [
    "- Document takeaways & any actions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb78677f-addb-4e3b-9e7e-a706c1e154f1",
   "metadata": {},
   "source": [
    "4. Multivariate Stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98136ca-d122-4e8a-aa20-eb546edd4309",
   "metadata": {},
   "source": [
    "- Visualize the interaction of each measurement type with the others using a pairplot (or scatter matrix or something similar) and add color to represent species."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216e199f-6620-473a-9289-ce0359443a85",
   "metadata": {},
   "source": [
    "- Visualize two numeric variables by means of the species. Hint: `sns.relplot` with `hue` or `col`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc33595-3983-40b9-8cab-c9fb65496459",
   "metadata": {},
   "source": [
    "- Create a swarmplot using a melted dataframe of all your numeric variables. The x-axis should be the variable name, the y-axis the measure. Add another dimension using color to represent species. Document takeaways from this visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b59ab66-b6f2-4e77-bbc2-13cc5415c45d",
   "metadata": {},
   "source": [
    "- Ask a specific question of the data, such as: is the sepal area signficantly different in virginica compared to setosa? Answer the question through both a plot and using a mann-whitney or t-test. If you use a t-test, be sure assumptions are met (independence, normality, equal variance)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43537a7b-319a-4d75-a2c1-82bf43f15046",
   "metadata": {},
   "source": [
    "- Document takeaways and any actions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f737d2-8ef9-469d-b0ee-27684b43bf8b",
   "metadata": {},
   "source": [
    "### Exercises Part II\n",
    "Explore your `titanic` dataset more completely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436deb23-9598-42e4-aff6-2b798d7980f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import acquire\n",
    "import prepare\n",
    "titanic_df = acquire.get_titanic_data()\n",
    "titanic_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de763377-56c1-4062-84da-f4e349445613",
   "metadata": {},
   "source": [
    "- Determine drivers of the target variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9451150e-cb02-4146-a033-2cd12af0e84d",
   "metadata": {},
   "source": [
    "- Determine if certain columns should be dropped"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a64ade-22f1-4ed2-a769-f0b4e02bb52e",
   "metadata": {},
   "source": [
    "- Determine if it would be valuable to bin some numeric columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd465cd3-b15a-4c86-8f32-dae8ad451981",
   "metadata": {},
   "source": [
    "- Determine if it would be valuable to combine multiple columns into one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd7ee01-5382-4b52-92eb-0b49efbb8778",
   "metadata": {},
   "source": [
    "Does it make sense to combine any features?\n",
    "\n",
    "Do you find any surprises?\n",
    "\n",
    "Document any and all findings and takeaways in your notebook using markdown.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8684fa-a89d-4290-85f9-061a60a47651",
   "metadata": {},
   "source": [
    "### Exercises Part III"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5819983b-4b7e-46d4-baca-fbdbe084ce81",
   "metadata": {},
   "source": [
    "- Explore your `telco` data to discover drivers of churn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686252a8-0990-4e20-b502-cddd140c1478",
   "metadata": {},
   "source": [
    "- Determine if certain columns should be dropped"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d295b425-9405-4c06-a870-4582eee26cb1",
   "metadata": {},
   "source": [
    "- Determine if it would be valuable to bin some numeric columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f745f741-b4bb-47b0-8537-aede0ea1bbdd",
   "metadata": {},
   "source": [
    "- Determine if it would be valuable to combine multiple columns into one.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02af775-de23-40b4-8cec-9b60bb7743b9",
   "metadata": {},
   "source": [
    "What are your drivers of churn?\n",
    "\n",
    "Does it make sense to combine any features?\n",
    "\n",
    "Do you find any surprises?\n",
    "\n",
    "Document any and all findings and takeaways in your notebook using markdown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5196a03-3ee1-4186-a1c0-234da1698300",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
